---
title: PAC-Bayesian Soft Actor-Critic Learning
abstract: Actor-critic algorithms address the dual goals of reinforcement learning
  (RL), policy evaluation and improvement via two separate function approximators.
  The practicality of this approach comes at the expense of training instability,
  caused mainly by the destructive effect of the approximation errors of the critic
  on the actor. We tackle this bottleneck by employing an existing Probably Approximately
  Correct (PAC) Bayesian bound for the first time as the critic training objective
  of the Soft Actor-Critic (SAC) algorithm. We further demonstrate that online learning
  performance improves significantly when a stochastic actor explores multiple futures
  by critic-guided random search. We observe our resulting algorithm to compare favorably
  against the state-of-the-art SAC implementation on multiple classical control and
  locomotion tasks in terms of both sample efficiency and regret.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tasdighi24a
month: 0
tex_title: PAC-Bayesian Soft Actor-Critic Learning
firstpage: 127
lastpage: 145
page: 127-145
order: 127
cycles: false
bibtex_author: Tasdighi, Bahareh and Akg{\"u}l, Abdullah and Haussmann, Manuel and
  Brink, Kenny Kazimirzak and Kandemir, Melih
author:
- given: Bahareh
  family: Tasdighi
- given: Abdullah
  family: Akg√ºl
- given: Manuel
  family: Haussmann
- given: Kenny Kazimirzak
  family: Brink
- given: Melih
  family: Kandemir
date: 2024-07-29
address:
container-title: Proceedings of the 6th Symposium on Advances in Approximate Bayesian
  Inference
volume: '253'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 29
pdf: https://raw.githubusercontent.com/mlresearch/v253/main/assets/tasdighi24a/tasdighi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
